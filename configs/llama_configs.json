{   
    "run_name": "pretrain",
    "output_dir": "models/llama32-1b/pretrain",
    "overwrite_output_dir": true,
    "report_to": "wandb",
    "eval_strategy": "epoch",
    "logging_steps": 50,
    "num_train_epochs": 0,
    "bf16": true,
    "seq_len": 768,
    "per_device_train_batch_size": 4,
    "learning_rate": 5e-5,
    "weight_decay": 5e-3,
    "warmup_ratio": 0.05,
    "lr_scheduler_type": "cosine"
}